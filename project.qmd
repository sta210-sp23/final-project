---
title: "Final Project - Predicting March Madness"
author: "Anmol Sapru and Rohit Gunda"
format: pdf
---

# [Introduction and Data]{.underline}

#### [Motivation]{.underline}

Duke is synonymous with basketball. As Duke students who love Duke Basketball and March Madness, we are interested in performing a statistical analysis on the most thrilling tournament in sports. While watching the 2023 March Madness tournament and the many upsets that came with it, we were motivated to see if we could use statistical methods to predict March Madness winners. Upon scouring sites such as FiveThirtyEight and the KenPom rankings, we were inspired to create models of our own to predict tournament success.

**Fundamental Research Question:** What variables are important to March Madness success and which outliers over the past 15 years have existed that bring "Madness" to "March?

#### [**Packages**]{.underline}

```{r load-packages, message = FALSE, warning = FALSE}

library(tidyverse)
library(tidymodels)
library(Stat2Data)
library(caret)
library(leaps)
library(MASS)
```

#### [Data]{.underline}

```{r load-data, echo = FALSE, message = FALSE}

cbb <- read_csv("data/20082022torvik.csv") 
sportsreference <- read_csv("data/sportsreference.csv")
background <- read_csv("data/teams_background.csv")

cbb <- left_join(cbb, background, by = c("TEAM" = "TEAM", "YEAR" = "YEAR"))

#Remove non-postseason teams and R68 losers
cbb <- cbb[!is.na(cbb$march_madness),]
cbb <- filter(cbb, !grepl("R68", march_madness))

#Cleaning up variable names, variables, etc
cbb$march_madness <- str_trim(cbb$march_madness, side = c("both"))
cbb <- rename(cbb, march_madness = march_madness) 

cbb <- cbb |>
  mutate(march_madness = case_when(
    march_madness == "Sweet Sixteen" ~ "S16",
    march_madness == "Elite Eight" ~ "E8",
    march_madness == "Final Four" ~ "F4",
    march_madness == "Finals" ~ "2ND",
    march_madness == "CHAMPS" ~ "Champions",
    TRUE ~ march_madness
  ))

cbb <- left_join(cbb, sportsreference, by = c("TEAM" = "School", "YEAR" = "Year"))

cbb <- cbb |>
  mutate(mm_WINS = case_when(march_madness == "R64" ~ 0, march_madness =="R32" ~ 1, 
                             march_madness == "E16" ~ 2, march_madness == "E8" ~ 3,
                             march_madness == "F4" ~ 4, march_madness == "2ND" ~ 5,
                             march_madness == "Champions" ~ 6, TRUE ~ 0))

```

Data was found from the [Sports Reference college basketball team stats website](https://www.sports-reference.com/cbb/seasons/men/2023-school-stats.html) and [Bart Torvik analytics website](https://barttorvik.com/#), with these general and deeper stats being taken going back to 2008 (excluding the cancelled March Madness of 2020). Both of these sources allow for easy copy + pasting of CSV files with the annual stats for each team and their tournament performance. From here, the data was cleaned in Excel to separate the year and result of each team then joined for all of this data together. With the nature of March Madness being over time, the data most definitely violates expectations of independence, with similar players, coaches, and more between years, but there would not be a reasonable way to complete such analysis without this independence.

#### [**Key Variables**]{.underline}

-   march_madness - Our key response variable that states the round each team was able to make it to in their tournament. Our overall goal is to predict this variable for teams in the 2023 March Madness Tournament

-   ADJOE/ADJDE - Points scored per 100 possessions on offense/defense, adjusted for opponent strength and game location

-   TOR/TORD - Turnovers committed/forced per game on offense/defense

-   ADJT - Estimate possessions per game a team would have against the average tempo

-   EFG% - Field goal percentage adjusted for value of baskets scored

#### [Description of Data Cleaning]{.underline}

For further cleaning, the non-March Madness teams added in the join were removed and simple variable names and values were cleaned up to be easier to work with. First we joined our original `cbb` dataset with `background`, both of which we got from Bart Torvik's analytics website. The latter dataset contained details about each team's performance in the tournament for the relevant years. The observations of teams that did not make the Round of 64 for their tournament were then removed to focus on further prediction.

We also abbreviated each of the outcomes in the `march_madness` variable. Then, we joined the `cbb` and the `sportsreference` datasets by team and year to aggregate all of the statistics and data that were interested in analyzing. For the round each team made it to, the values were shortened to a shorter form (i.e. F4 instead of Final Four). Each round was split up to its own variable and dataset which showed the success of each team in each round (e.g. win or loss in the Round of 32). Finally, we created the `mm_WINS` variable which tracks how many wins a given team had in a tournament year. We deduced this from the outcome from the `march_madness` variable, and applied it to our new variable. This helped us with our EDA as we were able to see how many wins each conference/team had in any given tournament.

#### [Exploratory Data Analysis]{.underline}

**MAYBE ADD ONE MORE EDA**

```{r EDA-conferences}

cbb |>
  mutate(mm_WINS = case_when(march_madness == "R64" ~ 0, march_madness =="R32" ~ 1, 
                             march_madness == "E16" ~ 2, march_madness == "E8" ~ 3,
                             march_madness == "F4" ~ 4, march_madness == "2ND" ~ 5,
                             march_madness == "Champions" ~ 6, TRUE ~ 0))

ggplot(cbb, aes(x = Conference, y = mm_WINS, fill = Conference)) +
  geom_col() +
  theme(legend.position = "none") +
  labs(
    x = "Conference",
    y = "Wins",
    title = "Major Conferences Dominate Wins in March Madness"
  )

```

```{r}

cbb |>
  subset(select = c(ADJOE, ADJDE, TOR, TORD, `ADJ T`, `EFG%`, march_madness)) |>
  group_by(march_madness) |>
  summarize(ADJOE = mean(ADJOE),
            ADJDE = mean(ADJDE),
            TOR = mean(TOR),
            TORD = mean(TORD),
            ADJT = mean(`ADJ T`),
            `EFG%` = mean(`EFG%`)) |>
  pivot_longer(cols = c(ADJOE, ADJDE, TOR, TORD, ADJT, `EFG%`)) |>
  subset(march_madness %in% c("R64", "S16", "F4", "Champions")) |>
  ggplot(aes(x = name, y = value, fill = march_madness)) +
  geom_col(position = "dodge") +
  labs(
    title = "Round-Wise Stats Trends for March Madness",
    x = "Key Variable",
    y = "Average by Round",
    legend = "March Madness Round"
  )
```

# [Methodology]{.underline}

We determined it would be best to try different regressions, with logistic regressions by round and an ordinal regression---each with their different assumptions to look into. The issue with the first of these is the low levels of data that limit the creation of a model beyond around the Elite Eight. In preparing for the round-by-round logistic regressions we split up the data from the original data sets into multiple data sets for each round, with a TRUE or FALSE value of whether they won their game in that round.

```{r round-separation, echo = FALSE}

#separated each row by round for determining differences
cbb_r <- mutate(cbb, round_64 = if_else(march_madness == "R64", FALSE, TRUE))
round_64 <- cbb_r

cbb_r <- mutate(cbb_r, round_32 = 
         case_when(march_madness == "R32" ~ FALSE,
         march_madness %in% c("S16", "E8", "F4", "2ND", "Champions") ~ TRUE,
         TRUE ~ NA))
round_32 <- cbb_r[!is.na(cbb_r$round_32),]

cbb_r <- mutate(cbb_r, sweet_sixteen = 
         case_when(march_madness == "S16" ~ FALSE,
         march_madness %in% c("E8", "F4", "2ND", "Champions") ~ TRUE,
         TRUE ~ NA))
sweet_sixteen <- cbb_r[!is.na(cbb_r$sweet_sixteen),]

cbb_r <- mutate(cbb_r, elite_eight = 
         case_when(march_madness == "E8" ~ FALSE,
         march_madness %in% c("F4", "2ND", "Champions") ~ TRUE,
         TRUE ~ NA))
elite_eight <- cbb_r[!is.na(cbb_r$elite_eight),]

cbb_r <- mutate(cbb_r, final_four = 
         case_when(march_madness == "F4" ~ FALSE,
         march_madness %in% c("2ND", "Champions") ~ TRUE,
         TRUE ~ NA))
final_four <- cbb_r[!is.na(cbb_r$final_four),]

cbb_r <- mutate(cbb_r, champ_game = 
         case_when(march_madness == "2ND" ~ FALSE,
         march_madness %in% c("Champions") ~ TRUE,
         TRUE ~ NA))
champ_game <- cbb_r[!is.na(cbb_r$champ_game),]
```

**WHY CHOSE EACH VARIABLE**

## [Round-by-Round Logistic Regression]{.underline}

The following is an example of the regression that was ran on all of the Round of 64 teams to create a regression that predicts winners (round_64 = TRUE) against losers (round_64 = FALSE) for the round. Using stepAIC works to attempt to limit the overfitting with the data. This was also done for the Round of 32 and Sweet Sixteen.

**WHY LOGISTIC REG?**

```{r round-64-model, results = "hide"}

round_64 <- na.omit(round_64)

round_64_max <- glm(round_64 ~ G.x + WINS + LOSSES + ADJOE + ADJDE + 
                      `EFG%` + `EFGD%` + TOR + TORD + ORB + DRB + FTR +
                      FTRD + `2P%` + `2P%D` + `3P%` + `3P%D` + `3PR` +
                      `3PRD` + `ADJ T` +`Conf. W-L%` + `Home W-L%` + 
                      `Away W-L%` + `AVG PPG` + `AVG DPPG` + `AVG PD` + 
                      `AST/TOV` + `PF/G` + WINS*G.x + LOSSES*G.x + 
                      ADJOE*ADJDE +`EFG%`*`EFGD%` + TOR*TORD + ORB*DRB + 
                      FTR*FTRD + `2P%`*`2P%D` + `3P%`*`3P%D` + `3PR`*`3PRD` +
                      `2P%`*`3P%` + `2P%D`*`3P%D` + `AVG PPG`*`AVG DPPG`,
                      data = round_64,
                      family = "binomial")

round_64_min <- glm(round_64 ~ 1,
                    data = round_64,
                    family = "binomial")

round_64_model <- stepAIC(round_64_max,
        scope = list(lower = round_64_min, upper = round_64_max),
        data = round_64, direction = "both")
```

```{r round-32-model, echo = FALSE, results = "hide"}

round_32_max <- glm(round_32 ~ G.x + WINS + LOSSES + ADJOE + ADJDE + 
                      `EFG%` + `EFGD%` + TOR + TORD + ORB + DRB + FTR +
                      FTRD + `2P%` + `2P%D` + `3P%` + `3P%D` + `3PR` +
                      `3PRD` + `ADJ T` +`Conf. W-L%` + `Home W-L%` + 
                      `Away W-L%` + `AVG PPG` + `AVG DPPG` + `AVG PD` + 
                      `AST/TOV` + `PF/G` + WINS*G.x + LOSSES*G.x + 
                      ADJOE*ADJDE +`EFG%`*`EFGD%` + TOR*TORD + ORB*DRB + 
                      FTR*FTRD + `2P%`*`2P%D` + `3P%`*`3P%D` + `3PR`*`3PRD` +
                      `2P%`*`3P%` + `2P%D`*`3P%D` + `AVG PPG`*`AVG DPPG`,
                    data = round_32,
                    family = "binomial")

round_32_min <- glm(round_32 ~ 1,
                    data = round_32,
                    family = "binomial")

round_32_model <- stepAIC(round_32_max,
        scope = list(lower = round_32_min, 
                     upper = round_32_max),
        data = round_32, direction = "both")
```

```{r sweet-sixteen-model, echo = FALSE, results = "hide"}

sweet_sixteen_max <- glm(sweet_sixteen ~ G.x + WINS + LOSSES + ADJOE + ADJDE + 
                      `EFG%` + `EFGD%` + TOR + TORD + ORB + DRB + FTR +
                      FTRD + `2P%` + `2P%D` + `3P%` + `3P%D` + `3PR` +
                      `3PRD` + `ADJ T` +`Conf. W-L%` + `Home W-L%` + 
                      `Away W-L%` + `AVG PPG` + `AVG DPPG` + `AVG PD` + 
                      `AST/TOV` + `PF/G` + WINS*G.x + LOSSES*G.x + 
                      ADJOE*ADJDE +`EFG%`*`EFGD%` + TOR*TORD + ORB*DRB + 
                      FTR*FTRD + `2P%`*`2P%D` + `3P%`*`3P%D` + `3PR`*`3PRD` +
                      `2P%`*`3P%` + `2P%D`*`3P%D` + `AVG PPG`*`AVG DPPG`,
                        data = sweet_sixteen,
                        family = "binomial")

sweet_sixteen_min <- glm(sweet_sixteen ~ 1,
                    data = sweet_sixteen,
                    family = "binomial")

sweet_sixteen_model <- stepAIC(sweet_sixteen_max,
        scope = list(lower = sweet_sixteen_min,
                     upper = sweet_sixteen_max),
        data = sweet_sixteen, direction = "both")
```

The following are the regression models that were found from the three rounds of data and regression models run through stepAIC:

## [Overall Ordinal Regression]{.underline}

We chose to complete an ordinal regression model as each of the next levels of the tournament are in an ordinal progession. While each round is a binary outcome (win/lose), the progression of each round is an ordinal progression.

```{r ordinal-reg}

cbb$march_madness <- fct_relevel(cbb$march_madness, 
                                 c("R64", "R32", "S16", "E8", 
                                   "F4", "2ND", "Champions"))

mmb_ord_model <- polr(march_madness ~ ADJOE + ADJDE + TORD + 
                        FTRD + `2P%D` + `3P%D` + `AVG PPG` + 
                        `AVG DPPG` + `PF/G`, data = cbb)
```

**ADD ASSESSMENT OF CONDITIONS + DIAGNOSTICS**

# [Results]{.underline}

```{r predictions}

`2023sr` <- read_csv("data/2023sportsreference.csv")
`2023analytics` <- read_csv("data/2023torvik.csv")

`2023stats` <- left_join(`2023analytics`, `2023sr`, by = c("TEAM" = "School"))

`2023teams` <- read_csv("data/2023teams.csv")

`2023stats` <- `2023stats` |>
  filter(`2023stats`$TEAM %in% `2023teams`$TEAM)

tibble(predict(round_64_model, `2023stats`)) |>
  mutate(rank = seq(1:64)) |>
  left_join(mutate(`2023stats`, rank = seq(1:64))) |>
  arrange(desc(predict(round_64_model, `2023stats`)))

tibble(predict(round_32_model, `2023stats`)) |>
  mutate(rank = seq(1:64)) |>
  left_join(mutate(`2023stats`, rank = seq(1:64))) |>
  arrange(desc(predict(round_32_model, `2023stats`)))

tibble(predict(sweet_sixteen_model, `2023stats`)) |>
  mutate(rank = seq(1:64)) |>
  left_join(mutate(`2023stats`, rank = seq(1:64))) |>
  arrange(desc(predict(sweet_sixteen_model, `2023stats`)))

tibble(predict(mmb_ord_model, `2023stats`)) |>
  mutate(rank = seq(1:64)) |>
  left_join(mutate(`2023stats`, rank = seq(1:64))) |>
  arrange(predict(mmb_ord_model, `2023stats`, type = "probs"))
```

**INTERPRET SOMETHING - MOST IMPORTANT VARIABLES**

**ADD ASSUMPTIONS**

**MODEL'S PREDICTIVE POWER**

## Outliers and Randomness of March Madness

# [Discussion]{.underline}

**ADD RESIDUAL PLOT + HISTOGRAM + QQ PLOT**

[Model Assumptions]{.underline}

Linearity:

Independence:

Constant Variance:

Normality:

**HYPOTHESIS TESTS**

**COOK'S DISTANCE**

**IDEA'S FOR FUTURE WORK**
